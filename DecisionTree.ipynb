{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPeOgxcsoKF5nZeoFJbUpSA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Decision Tree"],"metadata":{"id":"OBzRWXfuPG15"}},{"cell_type":"markdown","source":["Question 1: What is a Decision Tree, and how does it work in classification?\n","- A Decision Tree is a flowchart-like model used for classification. It works by making sequential splits on data features, creating branches that lead down to leaf nodes, which represent the final predicted class."],"metadata":{"id":"2gEDifnVLmj3"}},{"cell_type":"markdown","source":["Question 2: What are Gini Impurity and Entropy?\n","- They are both metrics used to measure the \"impurity\" or \"disorder\" of classes within a node. The tree uses them to decide which feature split will create the \"purest\" possible child nodes"],"metadata":{"id":"Mg9NAax4MIsB"}},{"cell_type":"markdown","source":["Question 3: What is the difference between Pre-Pruning and Post-Pruning?\n","- Pre-pruning stops the tree from growing during training (e.g., setting a max_depth), which is faster. Post-pruning lets the tree grow fully and then removes branches after training, which can lead to a more accurate model."],"metadata":{"id":"zsZ8bhKDMcWt"}},{"cell_type":"markdown","source":["Question 4: What is Information Gain?\n","- Information Gain is the measure of how much a feature split reduces impurity (like Gini or Entropy). It's important because the tree always chooses the split that provides the highest Information Gain."],"metadata":{"id":"m4Ig-hY5M5q_"}},{"cell_type":"markdown","source":["Question 5: What is a key advantage and limitation of Decision Trees?\n","- A key advantage is that they are highly interpretable and easy to understand (a \"white-box\" model). A key limitation is their tendency to overfit the training data if not pruned."],"metadata":{"id":"zcotk32MNENL"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zD1F_xX0gN1l","executionInfo":{"status":"ok","timestamp":1761482725772,"user_tz":-330,"elapsed":660,"user":{"displayName":"Aman Jat","userId":"03874188547701461901"}},"outputId":"68493ec0-2408-4983-8da8-9f8b807ff1e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model Accuracy: 1.0000\n","\n","Feature Importances:\n","             Feature  Importance\n","0  sepal length (cm)    0.000000\n","1   sepal width (cm)    0.019110\n","2  petal length (cm)    0.893264\n","3   petal width (cm)    0.087626\n"]}],"source":["# Question 6: Python code to load Iris, train with Gini, print accuracy and feature importances.\n","\n","import pandas as pd\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris Dataset\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","feature_names = iris.feature_names\n","\n","# Split data into training and testing sets to evaluate accuracy\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Train a Decision Tree Classifier using the Gini criterion\n","clf = DecisionTreeClassifier(criterion = 'gini', random_state = 42)\n","clf.fit(X_train, y_train)\n","\n","# Print the model's accuracy\n","y_pred = clf.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Model Accuracy: {accuracy:.4f}\\n\")\n","\n","# Print the feature importances\n","importances = clf.feature_importances_\n","feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n","print(\"Feature Importances:\")\n","print(feature_importance_df)"]},{"cell_type":"code","source":["# Question 7: Python code to load Iris, train with max_depth=3, and compare accuracy to a fully-grown tree.\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris Dataset\n","X, y = load_iris(return_X_y=True)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Train a fully-grown tree (max_depth=None is the default)\n","full_tree = DecisionTreeClassifier(random_state=42)\n","full_tree.fit(X_train, y_train)\n","y_pred_full = full_tree.predict(X_test)\n","acc_full = accuracy_score(y_test, y_pred_full)\n","\n","print(f\"Accuracy of Fully-Grown Tree: {acc_full:.4f}\")\n","print(f\"Depth of Fully-Grown Tree: {full_tree.get_depth()}\")\n","\n","# Train a Decision Tree Classifier with max_depth=3\n","pruned_tree = DecisionTreeClassifier(max_depth = 3, random_state= 42)\n","pruned_tree.fit(X_train, y_train)\n","y_pred_pruned = pruned_tree.predict(X_test)\n","acc_pruned = accuracy_score(y_test, y_pred_pruned)\n","\n","print(f\"\\nAccuracy of max_depth=3 Tree: {acc_pruned:.4f}\")\n","print(f\"\\nDepth of max_depth = 3 Tree: {pruned_tree.get_depth()}\")\n","\n","# Compare\n","print(f\"\\nComparison:\")\n","print(f\"The pruned (max_depth=3) tree achieved the same accuracy (1.0000) as the fully-grown tree.\")\n","print(\"This suggests the fully-grown tree might be slightly overfit, and the key decisions are all made in the first 3 levels.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B5PwIQ8ZjI_-","executionInfo":{"status":"ok","timestamp":1761483674200,"user_tz":-330,"elapsed":1112,"user":{"displayName":"Aman Jat","userId":"03874188547701461901"}},"outputId":"e1a7ed4a-a934-4572-99d7-501e682b8b92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of Fully-Grown Tree: 1.0000\n","Depth of Fully-Grown Tree: 6\n","\n","Accuracy of max_depth=3 Tree: 1.0000\n","\n","Depth of max_depth = 3 Tree: 3\n","\n","Comparison:\n","The pruned (max_depth=3) tree achieved the same accuracy (1.0000) as the fully-grown tree.\n","This suggests the fully-grown tree might be slightly overfit, and the key decisions are all made in the first 3 levels.\n"]}]},{"cell_type":"code","source":["# Question 8: Python code to load California Housing, train a regressor, print MSE and feature importances.\n","\n","import pandas as pd\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","\n","# Load the California Housing Dataset\n","housing = fetch_california_housing()\n","X = housing.data\n","y = housing.target\n","feature_names = housing.feature_names\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Train a Decision Tree Regressor\n","regressor = DecisionTreeRegressor(random_state=42)\n","regressor.fit(X_train, y_train)\n","\n","# Print the Mean Squared Error (MSE)\n","y_pred = regressor.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"Mean Squared Error (MSE): {mse:.4f}\\n\")\n","\n","# Print the feature importances\n","importances = regressor.feature_importances_\n","feature_importance_df = pd.DataFrame(\n","    {'Feature': feature_names, 'Importance': importances}\n",")\n","feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n","\n","print(\"Feature Importances:\")\n","print(feature_importance_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0VxgHr0vHIht","executionInfo":{"status":"ok","timestamp":1761492133036,"user_tz":-330,"elapsed":4002,"user":{"displayName":"Aman Jat","userId":"03874188547701461901"}},"outputId":"a974aa6b-070c-4c3a-9486-91f091b5b058"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Squared Error (MSE): 0.5280\n","\n","Feature Importances:\n","      Feature  Importance\n","0      MedInc    0.523456\n","5    AveOccup    0.139012\n","6    Latitude    0.089992\n","7   Longitude    0.088806\n","1    HouseAge    0.052135\n","2    AveRooms    0.049418\n","4  Population    0.032206\n","3   AveBedrms    0.024974\n"]}]},{"cell_type":"code","source":["# Question 9: Python code to load Iris, tune hyperparameters with GridSearchCV, print best params and accuracy.\n","\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris Dataset\n","X, y = load_iris(return_X_y=True)\n","\n","# Split data into train and test sets\n","# We tune on the training set and validate the final model on the test set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Tune hyperparameters using GridSearchCV\n","# Define the model\n","dt = DecisionTreeClassifier(random_state=42)\n","\n","# Define the parameter grid to search\n","param_grid = {\n","    'max_depth': [None, 3, 5, 7, 10],\n","    'min_samples_split': [2, 5, 10, 20],\n","    'min_samples_leaf': [1, 2, 5, 10],\n","    'criterion': ['gini', 'entropy']\n","}\n","\n","# Set up GridSearchCV with 5-fold cross-validation\n","grid_search = GridSearchCV(\n","    estimator=dt,\n","    param_grid=param_grid,\n","    cv=5,\n","    scoring='accuracy',\n","    n_jobs=-1  # Use all available cores\n",")\n","\n","# Run the grid search on the training data\n","grid_search.fit(X_train, y_train)\n","\n","# Print the best parameters\n","print(f\"Best Parameters found by GridSearchCV:\\n{grid_search.best_params_}\\n\")\n","\n","# Print the resulting model accuracy\n","# Get the best model found by the search\n","best_model = grid_search.best_estimator_\n","\n","# Evaluate the best model on the held-out test set\n","y_pred = best_model.predict(X_test)\n","final_accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Resulting Model Accuracy on Test Set: {final_accuracy:.4f}\")"],"metadata":{"id":"k-FcYd7cLALn","executionInfo":{"status":"ok","timestamp":1761593197959,"user_tz":-330,"elapsed":5394,"user":{"displayName":"Aman Jat","userId":"03874188547701461901"}},"outputId":"61363c42-83b1-4062-e801-0e6465424ed6","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters found by GridSearchCV:\n","{'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10}\n","\n","Resulting Model Accuracy on Test Set: 1.0000\n"]}]},{"cell_type":"markdown","source":["Question 10: Step-by-step process for the healthcare scenario.\n","\n","Here is the step-by-step process I would follow to build a predictive model for the healthcare company, along with the business value it provides.\n","\n","Step 1: Handle Missing Values\n","\n","Given a large dataset with mixed data types and missing values, preprocessing is critical.\n","\n","Analyze Missingness: First, I would visualize the missing data using a library like missingno to see if the missingness is random (MCAR) or follows a pattern (MAR/MNAR).\n","\n","Numerical Features (e.g., 'Age', 'BloodPressure'):\n","\n","Imputation: I would likely impute missing values using the median, which is robust to outliers, or using a more advanced method like KNNImputer, which fills in values based on the \"nearest\" (most similar) patients in the dataset.\n","\n","Categorical Features (e.g., 'BloodType', 'Symptom'):\n","\n","Imputation: I would impute missing values with the mode (the most frequent category).\n","\n","\"Missing\" as a Category: If the fact that data is \"missing\" is itself predictive (e.g., a test was \"not performed\"), I would create a new category called \"Missing\" or \"Unknown\".\n","\n","\n","Row/Column Deletion: Since the dataset is large, if a small number of patients are missing many features, I might drop those rows. If a specific feature is missing for >50% of patients, I might drop the entire feature column as it's unreliable.\n","\n","Step 2: Encode Categorical Features\n","\n","The scikit-learn Decision Tree model requires all inputs to be numeric.\n","\n","Ordinal Features (e.g., 'PainLevel' as 'Low', 'Medium', 'High'): I would use OrdinalEncoder to map these to integers that preserve their order (e.g., 0, 1, 2).\n","\n","Nominal Features (e.g., 'Gender', 'BloodType'): I would use OneHotEncoder (or pd.get_dummies). This creates new binary (0/1) columns for each category, preventing the model from assuming an incorrect order (e.g., that 'BloodType B' > 'BloodType A').\n","\n","Step 3: Train a Decision Tree Model\n","\n","Split Data: I would split the preprocessed data into a training set and a testing set (e.g., 80% train, 20% test) using train_test_split. I would also ensure stratify=y is set to maintain the same percentage of diseased and healthy patients in both splits, which is crucial for imbalanced medical data.\n","\n","Initial Training: I would train a default DecisionTreeClassifier on the training data (X_train, y_train) to establish a baseline performance.\n","\n","Step 4: Tune its Hyperparameters\n","\n","To prevent overfitting and find the best model, I would perform hyperparameter tuning using GridSearchCV or RandomizedSearchCV with 5-fold or 10-fold cross-validation on the training set.\n","\n","Parameters to Tune:\n","\n","criterion: ['gini', 'entropy']\n","\n","max_depth: [3, 5, 7, 10, None] (to control complexity)\n","\n","min_samples_split: [10, 20, 50] (to prevent splitting on small nodes)\n","\n","min_samples_leaf: [5, 10, 20] (to ensure leaves are not too specific)\n","\n","class_weight: ['balanced'] (This is critical if the disease is rare, as it will penalize the model more for misclassifying the minority 'diseased' class).\n","\n","Step 5: Evaluate its Performance\n","\n","After finding the best model from GridSearchCV, I would evaluate its performance on the held-out test set (X_test, y_test).\n","\n","Key Metrics: For disease prediction, accuracy is often misleading. The most important metrics would be:\n","\n","Confusion Matrix: To see the raw count of True Positives, True Negatives, False Positives, and False Negatives.\n","\n","Recall (Sensitivity): TP/(TP+FN). This is the most critical metric in this scenario. It answers: \"Of all the patients who actually have the disease, what percentage did our model correctly identify?\" We must minimize False Negatives (missing a sick patient), as the cost is extremely high.\n","\n","Precision: TP/(TP+FP). \"Of all the patients the model predicted as sick, how many were correct?\" This is important for avoiding unnecessary, costly, or invasive follow-up tests.\n","\n","F1-Score: The harmonic mean of Precision and Recall, providing a good balance.\n","\n","ROC-AUC: The Area Under the Curve shows how well the model can distinguish between the 'diseased' and 'healthy' classes.\n","\n","Real-World Business Value -\n","\n","This model would provide immense value to the healthcare company in several ways:\n","\n","Early-Stage Screening & Risk Stratification: The model can serve as a fast, low-cost, automated tool to screen a large patient population and flag individuals at high risk for the disease.\n","\n","Decision Support for Clinicians: It acts as a \"second opinion\" or decision-support tool for doctors. By showing why a patient is flagged (e.g., \"Age > 60 and Symptom-X = True\"), it helps doctors prioritize who needs immediate, and often more expensive, diagnostic testing (like MRIs or biopsies).\n","\n","Resource Optimization: By identifying high-risk patients more effectively, the company can allocate its limited resources (specialist appointments, diagnostic equipment, patient support programs) more efficiently to those who need them most.\n","\n","Interpretability and Trust: A key value of the Decision Tree is its \"white-box\" nature. Doctors and regulators can audit the model's logic. This transparency builds trust and allows clinical experts to validate that the rules the model learned are medically sound, which is a requirement that \"black-box\" models (like complex neural networks) struggle to meet."],"metadata":{"id":"_BDr3jZXS-8w"}}]}